1：频繁抓取网站Ip被封
	解决方案：需要一个代理IP库，定时更换代理Ip爬取网站，在爬取网站的时候可以设置一个睡眠时间，让爬虫的速度慢一点。
		Ip库，可以在网站搜集一些免费的，或者花钱买一些代理Ip。
2：针对抓取失败url如何处理，可能由于网络原因，第一次抓取失败，第二次再抓取就成功了。
	1：设置重试机制，如果url抓取失败，则把这个url放回到url队列中，重复三次，如果还是失败，则认为是无效链接。
		这样的话实现比较麻烦，需要把这个失败的url在sorted set集合中记录一下，把每个元素的分值当成重试的次数，
		大于三次的话就不把这个url放倒url链接库了。
	2：在这里我们直接使用httpclient的默认重试机制，默认三次，这样就可以了	
3：抓取的网站模版会不定期的变动
	把提取关键信息的表达式提取出来，保存到数据库中，每一个网站一套规则，不能写死在项目中，不然只要抓取的网站版本稍微变动就要重新修改发布项目。
	
	

		